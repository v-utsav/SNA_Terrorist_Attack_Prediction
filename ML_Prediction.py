# -*- coding: utf-8 -*-
"""ML_ SNA_Project.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/17U1kbFHztbuPfTvVnzIxksaQoo9Ik46J

Data Reading
"""

import pandas as pd

# load the excel file into a pandas DataFrame
df = pd.read_excel('/content/cleaned2.xlsx')

# print the first few rows of the DataFrame
#print(df.head())

"""filling Null values"""

df1=df.copy()

df['iyear'].unique()

len(df.loc[df['gname']=='Unknown','gname'])

len(df['gname'].unique())

df1

import pandas as pd

# Read data from CSV file
df1 = pd.read_excel('/content/cleaned2.xlsx')

#dropping the unknown group name since these will not be useful for our analysis

# Drop rows containing unknown terrorist organizations
df1 = df1[df1['gname'] != 'Unknown']




# Convert categorical variables to numerical values
df1 = pd.get_dummies(df1, columns=['country_txt', 'region_txt', 'gname','attacktype1_txt'])


# Perform feature scaling
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()



# Select relevant features
features = ['iyear', 'imonth', 'iday', 'latitude', 'longitude', 'success', 'nkill','gname', 'country_txt_Afghanistan', 'country_txt_Iraq','country_txt_India', 'region_txt_Middle East & North Africa', 'attacktype1_txt_Assassination', 'attacktype1_txt_Bombing/Explosion']

#df1 = df1[features]

df1.columns

k=['eventid','iday', 'country',
       'region', 'provstate', 'latitude', 'longitude','attacktype1','targtype1','natlty1', 'natlty1_txt','nkill',
       'weaptype1', 'weaptype1_txt']

import pandas as pd
import matplotlib.pyplot as plt

# Read data from CSV file
df = pd.read_excel('/content/cleaned2.xlsx')

# Plot the number of attacks by year
plt.figure(figsize=(12,6))
df.groupby('iyear').count().plot(kind='line')
plt.xlabel('Year')
plt.ylabel('Number of Attacks')
plt.title('Number of Terrorist Attacks by Year')
plt.show()

# Plot the number of attacks by country
plt.figure(figsize=(12,6))
df['country_txt'].value_counts().head(40).plot(kind='bar')
plt.xlabel('Country')
plt.ylabel('Number of Attacks')
plt.title('Top 20 Countries with the Most Terrorist Attacks')
plt.show()

# Plot the number of casualties by attack type
plt.figure(figsize=(12,6))
df.groupby('attacktype1_txt')['nkill'].sum().sort_values(ascending=False).plot(kind='bar')
plt.xlabel('Attack Type')
plt.ylabel('Number of Casualties')
plt.title('Number of Casualties by Attack Type')
plt.show()

df1.columns

l=[]
for i in df1.columns:
  if i in k:
    l.append(i)


from sklearn.preprocessing import LabelEncoder

# Create an instance of LabelEncoder
le = LabelEncoder()

# Encode the categorical feature
df1['targtype1_txt'] = le.fit_transform(df1['targtype1_txt'])

df1.[df1(l)]

df1['iyear'].groupby(df1['gname']).count().sort_values(ascending=False).iloc[0:10].plot.bar()
import matplotlib.pyplot as plt
plt.show()

gname=['Taliban',
'Islamic State of Iraq and the Levant (ISIL) ',         
'Shining Path (SL)'                ,                    
'Al-Shabaab'                      ,                    
'Houthi extremists (Ansar Allah)',                     
'Boko Haram'                                     ,     
"New People's Army (NPA)",                             
'Farabundo Marti National Liberation Front (FMLN)',  
'Irish Republican Army (IRA)',                       
"Kurdistan Workers' Party (PKK)" ]

df1=df1[df1.gname.isin(gname)]

df1.columns

df1.drop(['gname_Al-Shabaab', 'gname_Boko Haram',
       'gname_Farabundo Marti National Liberation Front (FMLN)',
       'gname_Houthi extremists (Ansar Allah)',
       'gname_Irish Republican Army (IRA)',
       "gname_Kurdistan Workers' Party (PKK)", "gname_New People's Army (NPA)",
       'gname_Shining Path (SL)', 'gname_Taliban'],axis=1,inplace=True)

y = df[df.gname.isin(gname)]['gname']

len(y)

df1.shape

X= df1.copy()

from sklearn.feature_selection import SelectKBest, chi2
import pandas as pd

# Read data from CSV file
#df1 = pd.read_excel('/content/cleaned2.xlsx')

# select the features and target
# select the features and target
X = df1.drop(['longitude','latitude'], axis=1)
#y = df1['gname']

# apply feature selection using SelectKBest and chi2
selector = SelectKBest(chi2, k=10)
X_new = selector.fit_transform(X, y)

# get the selected feature names and scores
selected_features = X.columns[selector.get_support()]
scores = selector.scores_[selector.get_support()]

# print the selected feature names and scores
print("Selected Features:")
for l, score in zip(selected_features, scores):
    print(l, 'score:', score)

import pandas as pd
from sklearn.model_selection import train_test_split

# Read data from CSV file
#df = pd.read_csv('global_terrorism_data.csv')

# Perform data cleaning and feature selection

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.1, random_state=42)

# Print the shape of the training and testing sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier

'''
# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

# Train a decision tree classifier on the training set
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Predict on the testing set
y_pred = clf.predict(X_test)


'''


# Train a random forest classifier
rf = RandomForestClassifier(n_estimators=500, random_state=42,)
rf.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = rf.predict(X_test)



# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Make predictions on the new data
print(y_pred)
len(y_pred)

df_y = pd.DataFrame(y_pred)

#def autopct(pct): # only show the label when it's > 10%
 #   return ('%.2f' % pct) if pct > 10 else ''


df_y.value_counts().plot(kind='pie', figsize=(28,12), autopct='%.2f')




#plot(kind='pie', figsize=(28,12), autopct='%.2f' % pct, labels=None)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
# %matplotlib inline


# Apply PCA
pca = PCA(n_components=5) # set the number of components to 5
principal_components = pca.fit_transform(df1)

# Get the top 5 features for each component
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index=df1.columns)
loadings['abs_PC1'] = loadings['PC1'].abs()
loadings['abs_PC2'] = loadings['PC2'].abs()
loadings['abs_PC3'] = loadings['PC3'].abs()
loadings['abs_PC4'] = loadings['PC4'].abs()
loadings['abs_PC5'] = loadings['PC5'].abs()

# Sort the loadings by absolute value for each component
loadings = loadings.sort_values(by='abs_PC1', ascending=False)
print(loadings[['PC1', 'abs_PC1']].head(5))

loadings = loadings.sort_values(by='abs_PC2', ascending=False)
print(loadings[['PC2', 'abs_PC2']].head(5))

loadings = loadings.sort_values(by='abs_PC3', ascending=False)
print(loadings[['PC3', 'abs_PC3']].head(5))

loadings = loadings.sort_values(by='abs_PC4', ascending=False)
print(loadings[['PC4', 'abs_PC4']].head(5))

loadings = loadings.sort_values(by='abs_PC5', ascending=False)
print(loadings[['PC5', 'abs_PC5']].head(5))

# Import required libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import seaborn as sns

# Load the dataset
#df = pd.read_csv('global_terrorism.csv', encoding='ISO-8859-1')

# Select features and target variable
#X = df[['nkill', 'nperps', 'success', 'suicide', 'weaptype1']]
#y = df['gname']

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Train a decision tree classifier on the training set
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Predict on the testing set
y_pred = clf.predict(X_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(cm)



from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
#cm = confusion_matrix(y_true, y_preds, normalize='all')
cmd = ConfusionMatrixDisplay(cm, display_labels=['gname_Al-Shabaab', 'gname_Boko Haram',
       'gname_Farabundo Marti National Liberation Front (FMLN)',
       'gname_Houthi extremists (Ansar Allah)',
       'gname_Irish Republican Army (IRA)',
       "gname_Kurdistan Workers' Party (PKK)", "gname_New People's Army (NPA)",
       'gname_Shining Path (SL)', 'gname_Taliban'])
cmd.plot()
plt.show()

"""--------------------------------------------------"""

import pandas as pd

# Read data from CSV file
df = pd.read_excel('/content/cleaned2.xlsx')



# Drop rows containing unknown terrorist organizations
df = df[df['gname'] != 'Unknown']

gname=['Taliban',
'Islamic State of Iraq and the Levant (ISIL) ',         
'Shining Path (SL)'                ,                    
'Al-Shabaab'                      ,                    
'Houthi extremists (Ansar Allah)',                     
'Boko Haram'                                     ,     
"New People's Army (NPA)",                             
'Farabundo Marti National Liberation Front (FMLN)',  
'Irish Republican Army (IRA)',                       
"Kurdistan Workers' Party (PKK)" ]

df=df[df.gname.isin(gname)]

df1 = df.copy()

# Convert categorical variables to numerical values
df1 = pd.get_dummies(df1, columns=['country_txt', 'region_txt','attacktype1_txt'])

# Select relevant features
features = ['iyear', 'imonth', 'iday', 'success', 'nkill', 'country_txt_Afghanistan', 'country_txt_Iraq','country_txt_India', 'region_txt_Middle East & North Africa', 'attacktype1_txt_Assassination', 'attacktype1_txt_Bombing/Explosion']

df1 = df1[features]

X = df1.copy()
y = df['gname']

from sklearn.feature_selection import SelectKBest, chi2

# apply feature selection using SelectKBest and chi2
selector = SelectKBest(chi2, k=10)
X_new = selector.fit_transform(X, y)

# get the selected feature names and scores
selected_features = X.columns[selector.get_support()]
scores = selector.scores_[selector.get_support()]

# print the selected feature names and scores
print("Selected Features:")
for feature, score in zip(selected_features, scores):
    print(feature, 'score:', score)

from sklearn.model_selection import train_test_split

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_new, y, test_size=0.1, random_state=42)

# Print the shape of the training and testing sets
print("X_train shape:", X_train.shape)
print("y_train shape:", y_train.shape)
print("X_test shape:", X_test.shape)
print("y_test shape:", y_test.shape)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier

# Train a random forest classifier
rf = RandomForestClassifier(n_estimators=500, random_state=42,)
rf.fit(X_train, y_train)

# Make predictions on the testing set
y_pred = rf.predict(X_test)



# Evaluate the performance of the classifier
accuracy = accuracy_score(y_test, y_pred)
precision = precision_score(y_test, y_pred, average='weighted')
recall = recall_score(y_test, y_pred, average='weighted')
f1 = f1_score(y_test, y_pred, average='weighted')

print("Accuracy:", accuracy)
print("Precision:", precision)
print("Recall:", recall)
print("F1 Score:", f1)

# Import required libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import seaborn as sns

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(cm)

# Apply Decision Tree
# Import required libraries
from sklearn.metrics import confusion_matrix
from sklearn.model_selection import train_test_split
from sklearn.tree import DecisionTreeClassifier
import pandas as pd
import seaborn as sns

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)

# Train a decision tree classifier on the training set
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Predict on the testing set
y_pred = clf.predict(X_test)

# Calculate the confusion matrix
cm = confusion_matrix(y_test, y_pred)

# Print the confusion matrix
print("Confusion Matrix:")
print(cm)

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.decomposition import PCA
# %matplotlib inline


# Apply PCA
pca = PCA(n_components=5) # set the number of components to 5
principal_components = pca.fit_transform(df1)

# Get the top 5 features for each component
loadings = pd.DataFrame(pca.components_.T, columns=['PC1', 'PC2', 'PC3', 'PC4', 'PC5'], index=df1.columns)
loadings['abs_PC1'] = loadings['PC1'].abs()
loadings['abs_PC2'] = loadings['PC2'].abs()
loadings['abs_PC3'] = loadings['PC3'].abs()
loadings['abs_PC4'] = loadings['PC4'].abs()
loadings['abs_PC5'] = loadings['PC5'].abs()

# Sort the loadings by absolute value for each component
loadings = loadings.sort_values(by='abs_PC1', ascending=False)
print(loadings[['PC1', 'abs_PC1']].head(5))

loadings = loadings.sort_values(by='abs_PC2', ascending=False)
print(loadings[['PC2', 'abs_PC2']].head(5))

loadings = loadings.sort_values(by='abs_PC3', ascending=False)
print(loadings[['PC3', 'abs_PC3']].head(5))

loadings = loadings.sort_values(by='abs_PC4', ascending=False)
print(loadings[['PC4', 'abs_PC4']].head(5))

loadings = loadings.sort_values(by='abs_PC5', ascending=False)
print(loadings[['PC5', 'abs_PC5']].head(5))

"""# **After** **midsem**


"""

import pandas as pd
import numpy as np
import networkx as nx

# Read data from CSV file
df = pd.read_excel('/content/cleaned2.xlsx')

# Create an undirected graph from the DataFrame
G = nx.Graph()
for i, row in df.iterrows():
    G.add_edge(row['country_txt'], row['gname'])

# Calculate the degree of each node
degree = dict(G.degree())

# Calculate the clustering coefficient of each node
clustering = nx.clustering(G)

# Calculate the shortest path distance between each pair of nodes
shortest_paths = dict(nx.all_pairs_shortest_path_length(G))

# Calculate the betweenness centrality of each node
betweenness = nx.betweenness_centrality(G)

# Calculate the weight of each edge
weights = {}
for i, row in df.iterrows():
    if (row['country_txt'], row['gname']) in weights:
        weights[(row['country_txt'], row['gname'])] += 1
    else:
        weights[(row['country_txt'], row['gname'])] = 1

# Create a new DataFrame to store the features
features = pd.DataFrame(index=G.nodes())

# Add the degree as a column
features['degree'] = pd.Series(degree)

# Add the clustering coefficient as a column
features['clustering'] = pd.Series(clustering)

# Add the betweenness centrality as a column
features['betweenness'] = pd.Series(betweenness)

# Add the weight of each edge as a column
features['weight'] = pd.Series(weights)

# Assign random weights to edges
for edge in G.edges():
    if edge not in weights:
        weights[edge] = np.random.randint(1, 10)

# Update the 'weight' column in the features DataFrame
features['weight'] = pd.Series(weights)

# Save the DataFrame to a new Excel file
features.to_excel('features2.xlsx')

import pandas as pd
import numpy as np

# Read the predicted edges from Excel file
predicted_edges_df = pd.read_csv('/content/final_predicted_links.csv')

# Convert the DataFrame to a list of tuples
predicted_edges = predicted_edges_df.to_records(index=False).tolist()

# Convert the list of tuples to a DataFrame
pred_edges = pd.DataFrame(predicted_edges, columns=['country_txt', 'gname'])

pred_edges

import pandas as pd
import networkx as nx

# Read the Excel file with gname, degree, clustering, and weight columns
df_gname = pd.read_excel('/content/features3.xlsx')

# Drop the rows where 'gname' is 'Unknown'
df_gname = df_gname[df_gname['gname'] != 'Unknown']



# Create an empty undirected graph
G = nx.Graph()

# Add nodes for each country and group
countries = set(df_gname['country_txt'])
groups = set(df_gname['gname'])
for c in countries:
    G.add_node(c)
for g in groups:
    G.add_node(g)

# Add edges for each country-group pair
for i, row in df.iterrows():
    G.add_edge(row['country_txt'], row['gname'])

# Calculate the desired network statistics
degrees = dict(nx.degree(G))
clustering = dict(nx.clustering(G))
betweenness = nx.betweenness_centrality(G)

# Add the network statistics to the DataFrame as new columns
df_gname['degree'] = df_gname['gname'].map(degrees)
df_gname['clustering'] = df_gname['gname'].map(clustering)
df_gname['betweenness'] = df_gname['gname'].map(betweenness)

# Save the updated DataFrame to a new Excel file
df.to_excel('features5.xlsx', index=False)

import pandas as pd

# Read the Excel file
df = pd.read_excel('/content/features5.xlsx')
df = df[df['degree'] != 1]
df = df[df['gname'] != 'Unknown']    # Filter out rows where degree is equal to 1


# Save the filtered data to a new Excel file
#df1('features6.xlsx', index=False)

df

df1=df.copy()

import pandas as pd
import matplotlib.pyplot as plt


# Plot degree vs gname
plt.scatter(df1['gname'], df1['degree'])
plt.xticks(rotation=90)
plt.xlabel('gname')
plt.ylabel('degree')
plt.show()

from sklearn.preprocessing import LabelEncoder

le = LabelEncoder()
df1['country_txt'] = le.fit_transform(df1['country_txt'])

from sklearn.model_selection import train_test_split

X = df1.drop(['gname'], axis=1)
y = df1['gname']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

df1.isnull().sum()

from sklearn.linear_model import LogisticRegression

clf = LogisticRegression()
clf.fit(X_train, y_train)

from sklearn.metrics import accuracy_score

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print('Accuracy:', accuracy)

import pandas as pd
from sklearn.tree import DecisionTreeClassifier


# Select the features you want to use for prediction
X = df1[['degree', 'clustering','country_txt']]
y = df1['gname']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)
# Train a decision tree classifier on your data
# Train the decision tree model
clf = DecisionTreeClassifier()
clf.fit(X_train, y_train)

# Predict the probabilities for each sample
probs = clf.predict_proba(X)

# Add the predicted probabilities to your DataFrame
df['prob_label'] = probs[:, 1]

# Save the updated DataFrame to a new Excel file
df.to_excel('probabilities.xlsx', index=False)

y_pred = clf.predict(X_test)
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy}")

import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Create a Random Forest Classifier with 100 trees
rfc = RandomForestClassifier(n_estimators=100)

# Fit the model on the training data
rfc.fit(X_train, y_train)

# Make predictions on the test data
y_pred = rfc.predict(X_test)

# Evaluate the model performance using accuracy score
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy score: {accuracy}")